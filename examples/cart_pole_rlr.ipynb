{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safe Learning for a Cart Pole System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "import safe_learning\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.linalg import block_diag\n",
    "from utilities import CartPole, compute_closedloop_response, get_parameter_change, find_nearest, reward_rollout, compute_roa, binary_cmap\n",
    "%matplotlib inline\n",
    "# Nice progress bars\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Options, self).__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "OPTIONS = Options(np_dtype              = safe_learning.config.np_dtype,\n",
    "                  tf_dtype              = safe_learning.config.dtype,\n",
    "                  saturate              = True,                            # apply saturation constraints to the control input\n",
    "                  eps                   = 1e-8,                            # numerical tolerance\n",
    "                  use_linear_dynamics   = False,                           # use the linearized form of the dynamics as the true dynamics (for testing)\n",
    "                  dpi                   = 200,\n",
    "                  num_cores             = 4,\n",
    "                  num_sockets           = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Session\n",
    "\n",
    "Customize the TensorFlow session for the current device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the session\n",
    "# Open a new session (close old one if exists)\n",
    "\n",
    "\n",
    "os.environ[\"KMP_BLOCKTIME\"]    = str(0)\n",
    "os.environ[\"KMP_SETTINGS\"]     = str(1)\n",
    "os.environ[\"KMP_AFFINITY\"]     = 'granularity=fine,noverbose,compact,1,0'\n",
    "os.environ[\"OMP_NUM_THREADS\"]  = str(OPTIONS.num_cores)\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads  = OPTIONS.num_cores,\n",
    "                        inter_op_parallelism_threads  = OPTIONS.num_sockets,\n",
    "                        allow_soft_placement          = False,\n",
    "                        device_count                  = {'CPU': OPTIONS.num_cores})\n",
    "\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "session = tf.InteractiveSession(config=config)\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define underlying dynamic system and costs/rewards\n",
    "Define the dynamics of the true and false system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.  2.]]\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "dt = 0.01   # sampling time\n",
    "g = 9.81    # gravity\n",
    "\n",
    "# True system parameters\n",
    "m = 0.2    # pendulum mass\n",
    "M = 1.0    # cart mass\n",
    "L = 0.5    # pole length\n",
    "b = 0.2     # rotational friction\n",
    "\n",
    "# 'Wrong' model parameters\n",
    "mw = 0.2    # pendulum mass\n",
    "Mw = 1.0    # cart mass\n",
    "Lw = 0.5    # pole length\n",
    "bw = 0.0    # rotational friction\n",
    "\n",
    "# State and action normalizers\n",
    "x_max         = 0.5                                 # linear position [m]\n",
    "theta_max     = np.deg2rad(120)                      # angular position [rad]\n",
    "x_dot_max     = 2                                   # linear velocity [m/s]\n",
    "theta_dot_max = np.deg2rad(30)                      # angular velocity [rad/s]\n",
    "u_max         = (m + M) * (x_dot_max ** 2) / x_max  # linear force [N], control action\n",
    "\n",
    "state_norm = (x_max, theta_max, x_dot_max, theta_dot_max)\n",
    "action_norm = (u_max,)\n",
    "\n",
    "# Dimensions and domains\n",
    "state_dim     = 4\n",
    "action_dim    = 1\n",
    "state_limits  = np.array([[-math.pi, math.pi]] * state_dim)\n",
    "action_limits = np.array([[-2., 2.]] * action_dim)\n",
    "print(action_limits)\n",
    "# Initialize system class and its linearization\n",
    "true_dynamics = CartPole(m, M, L, b, dt, [state_norm, action_norm])\n",
    "wrong_dynamics = CartPole(mw, Mw, Lw, bw, dt, [state_norm, action_norm])\n",
    "    \n",
    "# LQR cost matrices\n",
    "Q = 0.1 * np.identity(state_dim).astype(OPTIONS.np_dtype)     # state cost matrix\n",
    "R = 0.1 * np.identity(action_dim).astype(OPTIONS.np_dtype)    # action cost matrix\n",
    "\n",
    "# Quadratic reward/cost function\n",
    "reward_function = safe_learning.QuadraticFunction(block_diag(- Q, - R), name='reward_function')\n",
    "\n",
    "### Quadratic (LQR) reward function\n",
    "\n",
    "#reward_function = safe_learning.QuadraticFunction(linalg.block_diag(-q, -r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<safe_learning.functions.GridWorld object at 0x0000020D5A2F4080>\n",
      "Grid size: 6765201\n",
      "<safe_learning.functions.GridWorld object at 0x0000020D5A2F4048>\n",
      "Grid size: 160000\n"
     ]
    }
   ],
   "source": [
    "num_states = [51, 51, 51, 51]\n",
    "\n",
    "# State grid\n",
    "state_limits = np.array([[-1., 1.], ] * state_dim)\n",
    "\n",
    "safety_disc = safe_learning.GridWorld(state_limits, num_states)\n",
    "policy_disc = safe_learning.GridWorld(state_limits, [20, 20, 20, 20])\n",
    "\n",
    "# Discretization constant\n",
    "tau = np.min(safety_disc.unit_maxes)\n",
    "\n",
    "pivot_state = np.asarray([0., 0., 0., 0.], dtype=OPTIONS.np_dtype)\n",
    "\n",
    "# Snap pivot_state to the closest grid point\n",
    "pivot_index = np.zeros_like(pivot_state, dtype=int)\n",
    "for d in range(safety_disc.ndim):\n",
    "    pivot_index[d], pivot_state[d] = find_nearest(safety_disc.discrete_points[d], pivot_state[d])\n",
    "\n",
    "# Get 2d-planes of the discretization (x vs. v, theta vs. omega) according to pivot_state\n",
    "planes = [[1, 3], [0, 2]]\n",
    "safety_slices = []\n",
    "for p in planes:\n",
    "    safety_slices.append(np.logical_and(safety_disc.all_points[:, p[0]] == pivot_state[p[0]], \n",
    "                                      safety_disc.all_points[:, p[1]] == pivot_state[p[1]]).ravel())\n",
    "print(safety_disc)\n",
    "print('Grid size: {0}'.format(safety_disc.nindex))\n",
    "\n",
    "# Snap pivot_state to the closest grid point\n",
    "pivot_index = np.zeros_like(pivot_state, dtype=int)\n",
    "for d in range(policy_disc.ndim):\n",
    "    pivot_index[d], pivot_state[d] = find_nearest(policy_disc.discrete_points[d], pivot_state[d])\n",
    "\n",
    "# Get 2d-planes of the discretization (x vs. v, theta vs. omega) according to pivot_state\n",
    "planes = [[1, 3], [0, 2]]\n",
    "policy_slices = []\n",
    "for p in planes:\n",
    "    policy_slices.append(np.logical_and(policy_disc.all_points[:, p[0]] == pivot_state[p[0]], \n",
    "                                      policy_disc.all_points[:, p[1]] == pivot_state[p[1]]).ravel())\n",
    "print((policy_disc))\n",
    "print('Grid size: {0}'.format(policy_disc.nindex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LQR init policy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Estimate value functions and ROAs with rollout\n",
    "roa_horizon     = 2000\n",
    "rollout_horizon = 500\n",
    "roa_tol         = 0.1\n",
    "rollout_tol     = 0.01\n",
    "discount        = feed_dict[gamma]  # use the same discount factor from training!\n",
    "# LQR solution (\\pi and V_\\pi)\n",
    "closed_loop_dynamics = lambda x: future_states_lqr.eval({states: x})\n",
    "reward_eval          = lambda x: rewards_lqr.eval({states: x})\n",
    "true_values          = [reward_rollout(grid.all_points[mask], closed_loop_dynamics, reward_eval, discount, rollout_horizon, rollout_tol) for mask in grid_slices]\n",
    "true_roas            = [compute_roa(grid.all_points[mask], closed_loop_dynamics, roa_horizon, roa_tol) for mask in grid_slices]\n",
    "\n",
    "# Parametric policy's value function V_{\\pi_\\delta}\n",
    "closed_loop_dynamics = lambda x: future_states.eval({states: x})\n",
    "reward_eval          = lambda x: rewards.eval({states: x})\n",
    "est_values           = [reward_rollout(grid.all_points[mask], closed_loop_dynamics, reward_eval, discount, rollout_horizon, rollout_tol) for mask in grid_slices]\n",
    "est_roas             = [compute_roa(grid.all_points[mask], closed_loop_dynamics, roa_horizon, roa_tol) for mask in grid_slices]\n",
    "\n",
    "# Parametric value function V_\\theta\n",
    "par_values = [values.eval({states: grid.all_points[mask]}) for mask in grid_slices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the GP dynamics model\n",
    "\n",
    "We use a combination of kernels to model the errors in the dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "(4, 1)\n",
      "[[0.00000000e+00 4.22254267e-11 0.00000000e+00 4.25222878e-10\n",
      "  6.40175101e-12]\n",
      " [0.00000000e+00 3.46544747e-10 0.00000000e+00 3.48981091e-09\n",
      "  5.25392722e-11]\n",
      " [0.00000000e+00 2.35709519e-07 0.00000000e+00 1.04708054e-06\n",
      "  3.57356638e-08]\n",
      " [0.00000000e+00 4.95224771e-04 0.00000000e+00 2.19991208e-03\n",
      "  7.50804889e-05]]\n"
     ]
    }
   ],
   "source": [
    "A, B = wrong_dynamics.linearize()\n",
    "lipschitz_dynamics = 1\n",
    "print(np.shape(A))\n",
    "print(np.shape(B))\n",
    "noise_var = 0.001 ** 2\n",
    "\n",
    "m_true = np.hstack((true_dynamics.linearize()))\n",
    "m = np.hstack((A, B))\n",
    "\n",
    "variances = (m_true - m) ** 2\n",
    "print(variances)\n",
    "# Input to GP is of the form (x, u) = (state, action)\n",
    "full_dim = state_dim + action_dim\n",
    "# Make sure things remain \n",
    "np.clip(variances, 1e-5, None, out=variances)\n",
    "\n",
    "# Kernels (just linear for now)\n",
    "kernel1 = (gpflow.kernels.Linear(full_dim, variance=variances[0, :], ARD=True))\n",
    "          # + gpflow.kernels.Matern32(1, lengthscales=1, active_dims=[0])\n",
    "           #* gpflow.kernels.Linear(1, variance=variances[0, 3]))\n",
    "\n",
    "kernel2 = (gpflow.kernels.Linear(full_dim, variance=variances[1, :], ARD=True))\n",
    "          # + gpflow.kernels.Matern32(1, lengthscales=1, active_dims=[0])\n",
    "          # * gpflow.kernels.Linear(1, variance=variances[1, 3]))\n",
    "\n",
    "kernel3 = (gpflow.kernels.Linear(full_dim, variance=variances[2, :], ARD=True))\n",
    "          # + gpflow.kernels.Matern32(1, lengthscales=1, active_dims=[0])\n",
    "          # * gpflow.kernels.Linear(1, variance=variances[2, 3]))\n",
    "\n",
    "kernel4 = (gpflow.kernels.Linear(full_dim, variance=variances[3, :], ARD=True))\n",
    "          # + gpflow.kernels.Matern32(1, lengthscales=1, active_dims=[0])\n",
    "          # * gpflow.kernels.Linear(1, variance=variances[3, 3]))\n",
    "\n",
    "# Mean dynamics\n",
    "\n",
    "mean_dynamics = safe_learning.LinearSystem((A, B), name='mean_dynamics')\n",
    "mean_function1 = safe_learning.LinearSystem((A[[0], :], B[[0], :]), name='mean_dynamics_1')\n",
    "mean_function2 = safe_learning.LinearSystem((A[[1], :], B[[1], :]), name='mean_dynamics_2')\n",
    "mean_function3 = safe_learning.LinearSystem((A[[2], :], B[[2], :]), name='mean_dynamics_3')\n",
    "mean_function4 = safe_learning.LinearSystem((A[[3], :], B[[3], :]), name='mean_dynamics_4')\n",
    "\n",
    "# Define a GP model over the dynamics\n",
    "X_init   = np.zeros((1, full_dim), dtype=OPTIONS.np_dtype)\n",
    "Y_init   = np.zeros((1, 1), dtype=OPTIONS.np_dtype)\n",
    "\n",
    "gp1 = gpflow.gpr.GPR(X_init,\n",
    "                    Y_init,\n",
    "                    kernel1,\n",
    "                    mean_function=mean_function1)\n",
    "gp1.likelihood.variance = noise_var\n",
    "\n",
    "gp2 = gpflow.gpr.GPR(X_init,\n",
    "                    Y_init,\n",
    "                    kernel2,\n",
    "                    mean_function=mean_function2)\n",
    "gp2.likelihood.variance = noise_var\n",
    "\n",
    "gp3 = gpflow.gpr.GPR(X_init,\n",
    "                    Y_init,\n",
    "                    kernel3,\n",
    "                    mean_function=mean_function3)\n",
    "gp3.likelihood.variance = noise_var\n",
    "\n",
    "gp4 = gpflow.gpr.GPR(X_init,\n",
    "                    Y_init,\n",
    "                    kernel4,\n",
    "                    mean_function=mean_function4)\n",
    "gp4.likelihood.variance = noise_var\n",
    "\n",
    "\n",
    "gp1_fun = safe_learning.GaussianProcess(gp1)\n",
    "gp2_fun = safe_learning.GaussianProcess(gp2)\n",
    "gp3_fun = safe_learning.GaussianProcess(gp3)\n",
    "gp4_fun = safe_learning.GaussianProcess(gp4)\n",
    "\n",
    "dynamics = safe_learning.FunctionStack((gp1_fun, gp2_fun, gp3_fun, gp4_fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the optimal policy for the linear (and wrong) mean dynamics\n",
    "k, s = safe_learning.utilities.dlqr(A, B, Q, R)\n",
    "init_policy = safe_learning.LinearSystem((-k), name='initial_policy')\n",
    "init_policy = safe_learning.Saturation(init_policy, -1, 1)\n",
    "\n",
    "# Define the Lyapunov function corresponding to the initial policy\n",
    "init_lyapunov = safe_learning.QuadraticFunction(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the dynamic programming problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network policy\n",
    "relu = tf.nn.relu\n",
    "policy = safe_learning.NeuralNetwork(layers=[32, 32, 1],\n",
    "                                     nonlinearities=[relu, relu, tf.nn.tanh],\n",
    "                                     output_scale=action_limits[0, 1])\n",
    "\n",
    "# Define value function approximation\n",
    "value_function = safe_learning.Triangulation(policy_disc,\n",
    "                                             -init_lyapunov(policy_disc.all_points).eval(),\n",
    "                                             project=True)\n",
    "gamma = 0.98\n",
    "# Define policy optimization problem\n",
    "rl = safe_learning.PolicyIteration(\n",
    "    policy,\n",
    "    dynamics,\n",
    "    reward_function,\n",
    "    value_function,\n",
    "    gamma=gamma)\n",
    "    \n",
    "\n",
    "with tf.name_scope('rl_mean_optimization'):\n",
    "    rl_opt_value_function = rl.optimize_value_function()\n",
    "    \n",
    "    # Placeholder for states\n",
    "    tf_states_mean = tf.placeholder(OPTIONS.tf_dtype, [None, state_dim])\n",
    "    \n",
    "    # Optimize for expected gain\n",
    "    values = rl.future_values(tf_states_mean)\n",
    "    policy_loss = -tf.reduce_mean(values)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    adapt_policy_mean = optimizer.minimize(policy_loss, var_list=rl.policy.parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the session\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run initial dynamic programming for the mean dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    \n",
    "    # select random training batches\n",
    "    rl.feed_dict[tf_states_mean] = policy_disc.sample_continuous(100)\n",
    "\n",
    "    session.run(adapt_policy_mean, feed_dict=rl.feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Lyapunov function\n",
    "\n",
    "Here we use the fact that the optimal value function is a Lyapunov function for the optimal policy if the dynamics are deterministic. As uncertainty about the dynamics decreases, the value function for the mean dynamics will thus converge to a Lyapunov function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a discretization for safety verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyapunov_function = -rl.value_function\n",
    "lipschitz_lyapunov = lambda x: tf.reduce_max(tf.abs(rl.value_function.gradient(x)),\n",
    "                                             axis=1, keepdims=True)\n",
    "\n",
    "lipschitz_policy = lambda x: policy.lipschitz() \n",
    "\n",
    "a_true, b_true = true_dynamics.linearize()\n",
    "lipschitz_dynamics = lambda x: np.max(np.abs(a_true)) + np.max(np.abs(b_true)) * lipschitz_policy(x)\n",
    "\n",
    "# Lyapunov function definitial\n",
    "lyapunov = safe_learning.Lyapunov(safety_disc,\n",
    "                                  lyapunov_function,\n",
    "                                  dynamics,\n",
    "                                  lipschitz_dynamics,\n",
    "                                  lipschitz_lyapunov,\n",
    "                                  tau,\n",
    "                                  policy=rl.policy,\n",
    "                                  initial_set=None)\n",
    "\n",
    "# Set initial safe set (level set) based on initial Lyapunov candidate\n",
    "values = init_lyapunov(safety_disc.all_points).eval()\n",
    "cutoff = np.max(values) * 0.005\n",
    "\n",
    "lyapunov.initial_safe_set = np.squeeze(values, axis=1) <= cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_safe_set(lyapunov, show=True):\n",
    "    \"\"\"Plot the safe set for a given Lyapunov function.\"\"\"\n",
    "    plt.imshow(lyapunov.safe_set.reshape(num_states).T,\n",
    "               origin='lower',\n",
    "               extent=lyapunov.discretization.limits.ravel(),\n",
    "               vmin=0,\n",
    "               vmax=1)\n",
    "    \n",
    "    if isinstance(lyapunov.dynamics, safe_learning.UncertainFunction):\n",
    "        X = lyapunov.dynamics.functions[0].X\n",
    "        plt.plot(X[:, 0], X[:, 1], 'rx')\n",
    "    \n",
    "    plt.title('safe set')\n",
    "    plt.colorbar()\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "lyapunov.update_safe_set()\n",
    "#plot_safe_set(lyapunov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safe policy update\n",
    "\n",
    "We do dynamic programming, but enfore the decrease condition on the Lyapunov function using a Lagrange multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('policy_optimization'):\n",
    "    \n",
    "    # Placeholder for states\n",
    "    tf_states = tf.placeholder(safe_learning.config.dtype, [None, state_dim])\n",
    "    \n",
    "    # Add Lyapunov uncertainty (but only if safety-relevant)\n",
    "    values = rl.future_values(tf_states, lyapunov=lyapunov)\n",
    "    \n",
    "    policy_loss = -tf.reduce_mean(values)\n",
    "    \n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    adapt_policy = optimizer.minimize(policy_loss, var_list=rl.policy.parameters)\n",
    "    \n",
    "    \n",
    "def rl_optimize_policy(num_iter):\n",
    "    # Optimize value function\n",
    "    session.run(rl_opt_value_function, feed_dict=rl.feed_dict)\n",
    "\n",
    "    # select random training batches\n",
    "    for i in tqdm(range(num_iter)):\n",
    "        rl.feed_dict[tf_states] = lyapunov.discretization.sample_continuous(1000)\n",
    "\n",
    "        session.run(adapt_policy, feed_dict=rl.feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "\n",
    "We explore close to the current policy by sampling the most uncertain state that does not leave the current level set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_variation = np.array([[-0.02], [0.], [0.02]], dtype=OPTIONS.np_dtype)\n",
    "\n",
    "\n",
    "with tf.name_scope('add_new_measurement'):\n",
    "        tf_max_state_action = tf.placeholder(OPTIONS.tf_dtype,\n",
    "                                             shape=[1, full_dim])\n",
    "        tf_measurement = true_dynamics(tf_max_state_action)\n",
    "        \n",
    "def update_gp():\n",
    "    \"\"\"Update the GP model based on an actively selected data point.\"\"\"\n",
    "    # Get a new sample location\n",
    "    max_state_action, _ = safe_learning.get_safe_sample(lyapunov,\n",
    "                                                        action_variation,\n",
    "                                                        action_limits,\n",
    "                                                        num_samples=1000)\n",
    "\n",
    "    # Obtain a measurement of the true dynamics\n",
    "    lyapunov.feed_dict[tf_max_state_action] = max_state_action\n",
    "    measurement = tf_measurement.eval(feed_dict=lyapunov.feed_dict)\n",
    "\n",
    "    # Add the measurement to our GP dynamics\n",
    "    lyapunov.dynamics.add_data_point(max_state_action, measurement)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\problems\\problem.py:661: RuntimeWarning: overflow encountered in long_scalars\n",
      "  if self.max_big_small_squared < big*small**2:\n",
      "C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\problems\\problem.py:662: RuntimeWarning: overflow encountered in long_scalars\n",
      "  self.max_big_small_squared = big*small**2\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "MemoryError: \nTraceback (most recent call last):\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 206, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\safe_learning\\reinforcement_learning.py\", line 171, in _run_cvx_optimization\n    prob.solve(**solver_options)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\problems\\problem.py\", line 268, in solve\n    return solve_func(self, *args, **kwargs)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\problems\\problem.py\", line 381, in _solve\n    data, inverse_data = self._solving_chain.apply(self)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\chain.py\", line 65, in apply\n    problem, inv = r.apply(problem)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\qp2quad_form\\qp2symbolic_qp.py\", line 51, in apply\n    return super(Qp2SymbolicQp, self).apply(problem)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 44, in apply\n    constraint)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 74, in canonicalize_tree\n    canon_arg, c = self.canonicalize_tree(arg)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 74, in canonicalize_tree\n    canon_arg, c = self.canonicalize_tree(arg)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 74, in canonicalize_tree\n    canon_arg, c = self.canonicalize_tree(arg)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 77, in canonicalize_tree\n    canon_expr, c = self.canonicalize_expr(expr, canon_args)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 90, in canonicalize_expr\n    return Constant(expr.value), []\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\atom.py\", line 266, in value\n    return self._value_impl()\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\atom.py\", line 289, in _value_impl\n    result = self.numeric(arg_values)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\atom.py\", line 376, in new_numeric\n    result = numeric_func(self, values)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\affine\\promote.py\", line 67, in numeric\n    return np.ones(self.promoted_shape) * values[0]\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\", line 188, in ones\n    a = empty(shape, dtype, order)\n\nMemoryError\n\n\n\t [[node rl_mean_optimization/optimize_value_function/run_cvx_optimization (defined at C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\safe_learning\\utilities.py:65)  = PyFunc[Tin=[DT_DOUBLE, DT_DOUBLE], Tout=[DT_DOUBLE], token=\"pyfunc_0\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](function_stack/evaluate/stacked_mean, reward_function/evaluate/Sum)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'rl_mean_optimization/optimize_value_function/run_cvx_optimization', defined at:\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-78b7cfcb4847>\", line 22, in <module>\n    rl_opt_value_function = rl.optimize_value_function()\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\safe_learning\\utilities.py\", line 109, in wrapped_function\n    return function(*args, **kwargs)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\safe_learning\\reinforcement_learning.py\", line 209, in optimize_value_function\n    **solver_options)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\safe_learning\\utilities.py\", line 65, in wrapped_function\n    stateful=stateful, name=name)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 457, in py_func\n    func=func, inp=inp, Tout=Tout, stateful=stateful, eager=False, name=name)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 281, in _internal_py_func\n    input=inp, token=token, Tout=Tout, name=name)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\", line 131, in py_func\n    \"PyFunc\", input=input, token=token, Tout=Tout, name=name)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): MemoryError: \nTraceback (most recent call last):\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 206, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\safe_learning\\reinforcement_learning.py\", line 171, in _run_cvx_optimization\n    prob.solve(**solver_options)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\problems\\problem.py\", line 268, in solve\n    return solve_func(self, *args, **kwargs)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\problems\\problem.py\", line 381, in _solve\n    data, inverse_data = self._solving_chain.apply(self)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\chain.py\", line 65, in apply\n    problem, inv = r.apply(problem)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\qp2quad_form\\qp2symbolic_qp.py\", line 51, in apply\n    return super(Qp2SymbolicQp, self).apply(problem)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 44, in apply\n    constraint)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 74, in canonicalize_tree\n    canon_arg, c = self.canonicalize_tree(arg)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 74, in canonicalize_tree\n    canon_arg, c = self.canonicalize_tree(arg)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 74, in canonicalize_tree\n    canon_arg, c = self.canonicalize_tree(arg)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 77, in canonicalize_tree\n    canon_expr, c = self.canonicalize_expr(expr, canon_args)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 90, in canonicalize_expr\n    return Constant(expr.value), []\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\atom.py\", line 266, in value\n    return self._value_impl()\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\atom.py\", line 289, in _value_impl\n    result = self.numeric(arg_values)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\atom.py\", line 376, in new_numeric\n    result = numeric_func(self, values)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\affine\\promote.py\", line 67, in numeric\n    return np.ones(self.promoted_shape) * values[0]\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\", line 188, in ones\n    a = empty(shape, dtype, order)\n\nMemoryError\n\n\n\t [[node rl_mean_optimization/optimize_value_function/run_cvx_optimization (defined at C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\safe_learning\\utilities.py:65)  = PyFunc[Tin=[DT_DOUBLE, DT_DOUBLE], Tout=[DT_DOUBLE], token=\"pyfunc_0\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](function_stack/evaluate/stacked_mean, reward_function/evaluate/Sum)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: MemoryError: \nTraceback (most recent call last):\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 206, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\safe_learning\\reinforcement_learning.py\", line 171, in _run_cvx_optimization\n    prob.solve(**solver_options)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\problems\\problem.py\", line 268, in solve\n    return solve_func(self, *args, **kwargs)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\problems\\problem.py\", line 381, in _solve\n    data, inverse_data = self._solving_chain.apply(self)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\chain.py\", line 65, in apply\n    problem, inv = r.apply(problem)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\qp2quad_form\\qp2symbolic_qp.py\", line 51, in apply\n    return super(Qp2SymbolicQp, self).apply(problem)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 44, in apply\n    constraint)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 74, in canonicalize_tree\n    canon_arg, c = self.canonicalize_tree(arg)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 74, in canonicalize_tree\n    canon_arg, c = self.canonicalize_tree(arg)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 74, in canonicalize_tree\n    canon_arg, c = self.canonicalize_tree(arg)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 77, in canonicalize_tree\n    canon_expr, c = self.canonicalize_expr(expr, canon_args)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 90, in canonicalize_expr\n    return Constant(expr.value), []\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\atom.py\", line 266, in value\n    return self._value_impl()\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\atom.py\", line 289, in _value_impl\n    result = self.numeric(arg_values)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\atom.py\", line 376, in new_numeric\n    result = numeric_func(self, values)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\affine\\promote.py\", line 67, in numeric\n    return np.ones(self.promoted_shape) * values[0]\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\", line 188, in ones\n    a = empty(shape, dtype, order)\n\nMemoryError\n\n\n\t [[{{node rl_mean_optimization/optimize_value_function/run_cvx_optimization}} = PyFunc[Tin=[DT_DOUBLE, DT_DOUBLE], Tout=[DT_DOUBLE], token=\"pyfunc_0\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](function_stack/evaluate/stacked_mean, reward_function/evaluate/Sum)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-f8736eb03292>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# lyapunov.update_safe_set()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrl_optimize_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mrl_optimize_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlyapunov\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_safe_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-dcecfee8fdff>\u001b[0m in \u001b[0;36mrl_optimize_policy\u001b[1;34m(num_iter)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrl_optimize_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Optimize value function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrl_opt_value_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# select random training batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: MemoryError: \nTraceback (most recent call last):\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 206, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\safe_learning\\reinforcement_learning.py\", line 171, in _run_cvx_optimization\n    prob.solve(**solver_options)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\problems\\problem.py\", line 268, in solve\n    return solve_func(self, *args, **kwargs)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\problems\\problem.py\", line 381, in _solve\n    data, inverse_data = self._solving_chain.apply(self)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\chain.py\", line 65, in apply\n    problem, inv = r.apply(problem)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\qp2quad_form\\qp2symbolic_qp.py\", line 51, in apply\n    return super(Qp2SymbolicQp, self).apply(problem)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 44, in apply\n    constraint)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 74, in canonicalize_tree\n    canon_arg, c = self.canonicalize_tree(arg)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 74, in canonicalize_tree\n    canon_arg, c = self.canonicalize_tree(arg)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 74, in canonicalize_tree\n    canon_arg, c = self.canonicalize_tree(arg)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 77, in canonicalize_tree\n    canon_expr, c = self.canonicalize_expr(expr, canon_args)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 90, in canonicalize_expr\n    return Constant(expr.value), []\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\atom.py\", line 266, in value\n    return self._value_impl()\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\atom.py\", line 289, in _value_impl\n    result = self.numeric(arg_values)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\atom.py\", line 376, in new_numeric\n    result = numeric_func(self, values)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\affine\\promote.py\", line 67, in numeric\n    return np.ones(self.promoted_shape) * values[0]\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\", line 188, in ones\n    a = empty(shape, dtype, order)\n\nMemoryError\n\n\n\t [[node rl_mean_optimization/optimize_value_function/run_cvx_optimization (defined at C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\safe_learning\\utilities.py:65)  = PyFunc[Tin=[DT_DOUBLE, DT_DOUBLE], Tout=[DT_DOUBLE], token=\"pyfunc_0\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](function_stack/evaluate/stacked_mean, reward_function/evaluate/Sum)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'rl_mean_optimization/optimize_value_function/run_cvx_optimization', defined at:\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-78b7cfcb4847>\", line 22, in <module>\n    rl_opt_value_function = rl.optimize_value_function()\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\safe_learning\\utilities.py\", line 109, in wrapped_function\n    return function(*args, **kwargs)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\safe_learning\\reinforcement_learning.py\", line 209, in optimize_value_function\n    **solver_options)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\safe_learning\\utilities.py\", line 65, in wrapped_function\n    stateful=stateful, name=name)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 457, in py_func\n    func=func, inp=inp, Tout=Tout, stateful=stateful, eager=False, name=name)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 281, in _internal_py_func\n    input=inp, token=token, Tout=Tout, name=name)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\", line 131, in py_func\n    \"PyFunc\", input=input, token=token, Tout=Tout, name=name)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): MemoryError: \nTraceback (most recent call last):\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 206, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\safe_learning\\reinforcement_learning.py\", line 171, in _run_cvx_optimization\n    prob.solve(**solver_options)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\problems\\problem.py\", line 268, in solve\n    return solve_func(self, *args, **kwargs)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\problems\\problem.py\", line 381, in _solve\n    data, inverse_data = self._solving_chain.apply(self)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\chain.py\", line 65, in apply\n    problem, inv = r.apply(problem)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\qp2quad_form\\qp2symbolic_qp.py\", line 51, in apply\n    return super(Qp2SymbolicQp, self).apply(problem)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 44, in apply\n    constraint)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 74, in canonicalize_tree\n    canon_arg, c = self.canonicalize_tree(arg)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 74, in canonicalize_tree\n    canon_arg, c = self.canonicalize_tree(arg)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 74, in canonicalize_tree\n    canon_arg, c = self.canonicalize_tree(arg)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 77, in canonicalize_tree\n    canon_expr, c = self.canonicalize_expr(expr, canon_args)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\reductions\\canonicalization.py\", line 90, in canonicalize_expr\n    return Constant(expr.value), []\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\atom.py\", line 266, in value\n    return self._value_impl()\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\atom.py\", line 289, in _value_impl\n    result = self.numeric(arg_values)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\atom.py\", line 376, in new_numeric\n    result = numeric_func(self, values)\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\affine\\promote.py\", line 67, in numeric\n    return np.ones(self.promoted_shape) * values[0]\n\n  File \"C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\", line 188, in ones\n    a = empty(shape, dtype, order)\n\nMemoryError\n\n\n\t [[node rl_mean_optimization/optimize_value_function/run_cvx_optimization (defined at C:\\Users\\petar\\Anaconda3\\lib\\site-packages\\safe_learning\\utilities.py:65)  = PyFunc[Tin=[DT_DOUBLE, DT_DOUBLE], Tout=[DT_DOUBLE], token=\"pyfunc_0\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](function_stack/evaluate/stacked_mean, reward_function/evaluate/Sum)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "# lyapunov.update_safe_set()\n",
    "rl_optimize_policy(num_iter=100)\n",
    "rl_optimize_policy(num_iter=100)\n",
    "\n",
    "lyapunov.update_safe_set()\n",
    "#plot_safe_set(lyapunov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print('iteration {} with c_max: {}'.format(i, lyapunov.feed_dict[lyapunov.c_max]))\n",
    "    for i in tqdm(range(10)):\n",
    "        update_gp()\n",
    "    \n",
    "    rl_optimize_policy(num_iter=200)\n",
    "    lyapunov.update_values()\n",
    "    \n",
    "    # Update safe set and plot\n",
    "    lyapunov.update_safe_set()\n",
    "    #plot_safe_set(lyapunov)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot trajectories and analyse improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([[math.pi*0.5, -0.0, 0.5, 0.0]])\n",
    "\n",
    "states_new, actions_new = safe_learning.utilities.compute_trajectory(true_dynamics, rl.policy, x0, 500)\n",
    "states_old, actions_old = safe_learning.utilities.compute_trajectory(true_dynamics, init_policy, x0, 500)\n",
    "\n",
    "t = np.arange(len(states_new)) * true_dynamics.dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, states_new[:, 0], label='new')\n",
    "plt.plot(t, states_old[:, 0], label='old')\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('angle [rad]')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(t, states_new[:, 1], label='new')\n",
    "plt.plot(t, states_old[:, 1], label='old')\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('angular velocity [rad/s]')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t[:-1], actions_new, label='new')\n",
    "plt.plot(t[:-1], actions_old, label='old')\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('actions')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('reward old:', tf.reduce_sum(rl.reward_function(states_old[:-1], actions_old)).eval(feed_dict=rl.feed_dict))\n",
    "print('reward new:', tf.reduce_sum(rl.reward_function(states_new[:-1], actions_new)).eval(feed_dict=rl.feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
