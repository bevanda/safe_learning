{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safe Learning for a Mass-Spring-Damper System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MassSpringDamper'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fe201497a9ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msafe_learning\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msafe_learning\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMassSpringDamper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msafe_learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'MassSpringDamper'"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal, linalg\n",
    "from utilities import MassSpringDamper, compute_closedloop_response, get_parameter_change, find_nearest, reward_rollout, compute_roa\n",
    "#from compute import MassSpringDamper\n",
    "import math\n",
    "\n",
    "# Nice progress bars\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x\n",
    "    \n",
    "from safe_learning import *\n",
    "#from safe_learning import MassSpringDamper\n",
    "import safe_learning\n",
    "%matplotlib inline\n",
    "\n",
    "# Open a new session (close old one if exists)\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define underlying dynamic system and costs/rewards\n",
    "Define the dynamics of the true and false system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "m = 1\n",
    "\n",
    "# 'Wrong' model parameters\n",
    "mass = 0.8\n",
    "friction = 0.2\n",
    "elastic_const = 0.8\n",
    "gravity = 9.81\n",
    "inertia = mass\n",
    "\n",
    "# True model parameters\n",
    "true_mass = 1.0\n",
    "true_friction = 0.0\n",
    "true_elast_const = 1.0\n",
    "\n",
    "# Input saturation\n",
    "x_max = 3\n",
    "u_max = gravity * true_mass\n",
    "\n",
    "# Normalization\n",
    "norm_state = np.array([x_max, 0])\n",
    "norm_action = np.array([u_max])\n",
    "\n",
    "# Corresponding dynamic systems\n",
    "true_dynamics = MassSpringDamper(mass=true_mass, elastic_const=true_elast_const, friction=true_friction,\n",
    "                                 normalization=(norm_state, norm_action))\n",
    "\n",
    "wrong_dynamics = MassSpringDamper(mass=mass, elastic_const=elastic_const, friction=friction,\n",
    "                                  normalization=(norm_state, norm_action))\n",
    "\n",
    "# LQR cost matrices\n",
    "q = 1 * np.diag([1., 2.])\n",
    "r = 1.2 * np.array([[1]], dtype=safe_learning.config.np_dtype)\n",
    "\n",
    "# Quadratic (LQR) reward function\n",
    "gamma = 0.98\n",
    "reward_function = safe_learning.QuadraticFunction(linalg.block_diag(-q, -r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_min, x_max, discretization\\\n",
    "state_limits = np.array([[-2, 2], [-2, 2]])\n",
    "action_limits = np.array([[-1.5, 1.5]])\n",
    "num_states = [2001, 1501]\n",
    "\n",
    "safety_disc = safe_learning.GridWorld(state_limits, num_states)\n",
    "policy_disc = safe_learning.GridWorld(state_limits, [55, 55])\n",
    "\n",
    "# Discretization constant\n",
    "tau = np.min(safety_disc.unit_maxes)\n",
    "print((safety_disc))\n",
    "print('Grid size: {0}'.format(safety_disc.nindex))\n",
    "print(np.shape(policy_disc))\n",
    "print('Grid size: {0}'.format(policy_disc.nindex))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the GP dynamics model\n",
    "\n",
    "We use a combination of kernels to model the errors in the dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = wrong_dynamics.linearize()\n",
    "print(np.shape(A))\n",
    "print(np.shape(B))\n",
    "lipschitz_dynamics = 1\n",
    "\n",
    "noise_var = 0.001 ** 2\n",
    "\n",
    "m_true = np.hstack((true_dynamics.linearize()))\n",
    "m = np.hstack((A, B))\n",
    "\n",
    "variances = (m_true - m) ** 2\n",
    "print(np.shape(variances))\n",
    "# Make sure things remain \n",
    "np.clip(variances, 1e-5, None, out=variances)\n",
    "\n",
    "# Kernels\n",
    "kernel1 = (gpflow.kernels.Linear(3, variance=variances[0, :], ARD=True)\n",
    "           + gpflow.kernels.Matern32(1, lengthscales=1, active_dims=[0])\n",
    "           * gpflow.kernels.Linear(1, variance=variances[0, 1]))\n",
    "\n",
    "kernel2 = (gpflow.kernels.Linear(3, variance=variances[1, :], ARD=True)\n",
    "           + gpflow.kernels.Matern32(1, lengthscales=1, active_dims=[0])\n",
    "           * gpflow.kernels.Linear(1, variance=variances[1, 1]))\n",
    "\n",
    "# Mean dynamics\n",
    "\n",
    "mean_dynamics = safe_learning.LinearSystem((A, B), name='mean_dynamics')\n",
    "mean_function1 = safe_learning.LinearSystem((A[[0], :], B[[0], :]), name='mean_dynamics_1')\n",
    "mean_function2 = safe_learning.LinearSystem((A[[1], :], B[[1], :]), name='mean_dynamics_2')\n",
    "\n",
    "# Define a GP model over the dynamics\n",
    "gp1 = gpflow.gpr.GPR(np.empty((0, 3), dtype=safe_learning.config.np_dtype),\n",
    "                    np.empty((0, 1), dtype=safe_learning.config.np_dtype),\n",
    "                    kernel1,\n",
    "                    mean_function=mean_function1)\n",
    "gp1.likelihood.variance = noise_var\n",
    "\n",
    "gp2 = gpflow.gpr.GPR(np.empty((0, 3), dtype=safe_learning.config.np_dtype),\n",
    "                    np.empty((0, 1), dtype=safe_learning.config.np_dtype),\n",
    "                    kernel2,\n",
    "                    mean_function=mean_function2)\n",
    "gp2.likelihood.variance = noise_var\n",
    "\n",
    "gp1_fun = safe_learning.GaussianProcess(gp1)\n",
    "gp2_fun = safe_learning.GaussianProcess(gp2)\n",
    "\n",
    "dynamics = safe_learning.FunctionStack((gp1_fun, gp2_fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the optimal policy for the linear (and wrong) mean dynamics\n",
    "k, s = safe_learning.utilities.dlqr(A, B, q, r)\n",
    "init_policy = safe_learning.LinearSystem((-k), name='initial_policy')\n",
    "init_policy = safe_learning.Saturation(init_policy, -1, 1)\n",
    "\n",
    "# Define the Lyapunov function corresponding to the initial policy\n",
    "init_lyapunov = safe_learning.QuadraticFunction(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the dynamic programming problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network policy\n",
    "relu = tf.nn.relu\n",
    "policy = safe_learning.NeuralNetwork(layers=[32, 32, 1],\n",
    "                                     nonlinearities=[relu, relu, tf.nn.tanh],\n",
    "                                     output_scale=action_limits[0, 1])\n",
    "\n",
    "# Define value function approximation\n",
    "value_function = safe_learning.Triangulation(policy_disc,\n",
    "                                             -init_lyapunov(policy_disc.all_points).eval(),\n",
    "                                             project=True)\n",
    "\n",
    "# Define policy optimization problem\n",
    "rl = safe_learning.PolicyIteration(\n",
    "    policy,\n",
    "    dynamics,\n",
    "    reward_function,\n",
    "    value_function,\n",
    "    gamma=gamma)\n",
    "    \n",
    "\n",
    "with tf.name_scope('rl_mean_optimization'):\n",
    "    rl_opt_value_function = rl.optimize_value_function()\n",
    "    \n",
    "    # Placeholder for states\n",
    "    tf_states_mean = tf.placeholder(safe_learning.config.dtype, [None, 2])\n",
    "    \n",
    "    # Optimize for expected gain\n",
    "    values = rl.future_values(tf_states_mean)\n",
    "    policy_loss = -tf.reduce_mean(values)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    adapt_policy_mean = optimizer.minimize(policy_loss, var_list=rl.policy.parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the session\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run initial dynamic programming for the mean dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(3000)):\n",
    "    \n",
    "    # select random training batches\n",
    "    rl.feed_dict[tf_states_mean] = policy_disc.sample_continuous(1000)\n",
    "\n",
    "    session.run(adapt_policy_mean, feed_dict=rl.feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Lyapunov function\n",
    "\n",
    "Here we use the fact that the optimal value function is a Lyapunov function for the optimal policy if the dynamics are deterministic. As uncertainty about the dynamics decreases, the value function for the mean dynamics will thus converge to a Lyapunov function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a discretization for safety verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyapunov_function = -rl.value_function\n",
    "lipschitz_lyapunov = lambda x: tf.reduce_max(tf.abs(rl.value_function.gradient(x)),\n",
    "                                             axis=1, keepdims=True)\n",
    "\n",
    "lipschitz_policy = lambda x: policy.lipschitz() \n",
    "\n",
    "a_true, b_true = true_dynamics.linearize()\n",
    "lipschitz_dynamics = lambda x: np.max(np.abs(a_true)) + np.max(np.abs(b_true)) * lipschitz_policy(x)\n",
    "\n",
    "# Lyapunov function definitial\n",
    "lyapunov = safe_learning.Lyapunov(safety_disc,\n",
    "                                  lyapunov_function,\n",
    "                                  dynamics,\n",
    "                                  lipschitz_dynamics,\n",
    "                                  lipschitz_lyapunov,\n",
    "                                  tau,\n",
    "                                  policy=rl.policy,\n",
    "                                  initial_set=None)\n",
    "\n",
    "# Set initial safe set (level set) based on initial Lyapunov candidate\n",
    "values = init_lyapunov(safety_disc.all_points).eval()\n",
    "cutoff = np.max(values) * 0.005\n",
    "\n",
    "lyapunov.initial_safe_set = np.squeeze(values, axis=1) <= cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_safe_set(lyapunov, show=True):\n",
    "    \"\"\"Plot the safe set for a given Lyapunov function.\"\"\"\n",
    "    plt.imshow(lyapunov.safe_set.reshape(num_states).T,\n",
    "               origin='lower',\n",
    "               extent=lyapunov.discretization.limits.ravel(),\n",
    "               vmin=0,\n",
    "               vmax=1)\n",
    "    \n",
    "    if isinstance(lyapunov.dynamics, safe_learning.UncertainFunction):\n",
    "        X = lyapunov.dynamics.functions[0].X\n",
    "        plt.plot(X[:, 0], X[:, 1], 'rx')\n",
    "    \n",
    "    plt.title('safe set')\n",
    "    plt.colorbar()\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "lyapunov.update_safe_set()\n",
    "plot_safe_set(lyapunov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safe policy update\n",
    "\n",
    "We do dynamic programming, but enfore the decrease condition on the Lyapunov function using a Lagrange multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('policy_optimization'):\n",
    "    \n",
    "    # Placeholder for states\n",
    "    tf_states = tf.placeholder(safe_learning.config.dtype, [None, 2])\n",
    "    \n",
    "    # Add Lyapunov uncertainty (but only if safety-relevant)\n",
    "    values = rl.future_values(tf_states, lyapunov=lyapunov)\n",
    "    \n",
    "    policy_loss = -tf.reduce_mean(values)\n",
    "    \n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    adapt_policy = optimizer.minimize(policy_loss, var_list=rl.policy.parameters)\n",
    "    \n",
    "    \n",
    "def rl_optimize_policy(num_iter):\n",
    "    # Optimize value function\n",
    "    session.run(rl_opt_value_function, feed_dict=rl.feed_dict)\n",
    "\n",
    "    # select random training batches\n",
    "    for i in tqdm(range(num_iter)):\n",
    "        rl.feed_dict[tf_states] = lyapunov.discretization.sample_continuous(1000)\n",
    "\n",
    "        session.run(adapt_policy, feed_dict=rl.feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "We explore close to the current policy by sampling the most uncertain state that does not leave the current level set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_variation = np.array([[-0.02], [0.], [0.02]], dtype=safe_learning.config.np_dtype)\n",
    "\n",
    "\n",
    "with tf.name_scope('add_new_measurement'):\n",
    "        action_dim = lyapunov.policy.output_dim\n",
    "        tf_max_state_action = tf.placeholder(safe_learning.config.dtype,\n",
    "                                             shape=[1, safety_disc.ndim + action_dim])\n",
    "        tf_measurement = true_dynamics(tf_max_state_action)\n",
    "        \n",
    "def update_gp():\n",
    "    \"\"\"Update the GP model based on an actively selected data point.\"\"\"\n",
    "    # Get a new sample location\n",
    "    max_state_action, _ = safe_learning.get_safe_sample(lyapunov,\n",
    "                                                        action_variation,\n",
    "                                                        action_limits,\n",
    "                                                        num_samples=1000)\n",
    "\n",
    "    # Obtain a measurement of the true dynamics\n",
    "    lyapunov.feed_dict[tf_max_state_action] = max_state_action\n",
    "    measurement = tf_measurement.eval(feed_dict=lyapunov.feed_dict)\n",
    "\n",
    "    # Add the measurement to our GP dynamics\n",
    "    lyapunov.dynamics.add_data_point(max_state_action, measurement)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lyapunov.update_safe_set()\n",
    "rl_optimize_policy(num_iter=500)\n",
    "rl_optimize_policy(num_iter=500)\n",
    "\n",
    "lyapunov.update_safe_set()\n",
    "plot_safe_set(lyapunov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print('iteration {} with c_max: {}'.format(i, lyapunov.feed_dict[lyapunov.c_max]))\n",
    "    for i in tqdm(range(10)):\n",
    "        update_gp()\n",
    "    \n",
    "    rl_optimize_policy(num_iter=200)\n",
    "    lyapunov.update_values()\n",
    "    \n",
    "    # Update safe set and plot\n",
    "    lyapunov.update_safe_set()\n",
    "    plot_safe_set(lyapunov)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot trajectories and analyse improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([[0.0, 0.0]])\n",
    "\n",
    "states_new, actions_new = safe_learning.utilities.compute_trajectory(true_dynamics, rl.policy, x0, 500)\n",
    "states_old, actions_old = safe_learning.utilities.compute_trajectory(true_dynamics, init_policy, x0, 500)\n",
    "\n",
    "t = np.arange(len(states_new)) * true_dynamics.dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, states_new[:, 0], label='new')\n",
    "plt.plot(t, states_old[:, 0], label='old')\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('angle [rad]')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(t, states_new[:, 1], label='new')\n",
    "plt.plot(t, states_old[:, 1], label='old')\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('angular velocity [rad/s]')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t[:-1], actions_new, label='new')\n",
    "plt.plot(t[:-1], actions_old, label='old')\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('actions')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('reward old:', tf.reduce_sum(rl.reward_function(states_old[:-1], actions_old)).eval(feed_dict=rl.feed_dict))\n",
    "print('reward new:', tf.reduce_sum(rl.reward_function(states_new[:-1], actions_new)).eval(feed_dict=rl.feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
